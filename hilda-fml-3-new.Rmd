---
title: "Assignment_3_"
author:"Hilda Gandu"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#summary
The "Assignment_3_" R script examines a dataset containing information about automotive accidents in the United States in 2001. Based on the information available, its main objective is to determine whether or not an accident will result in injury. The following sections can be found in the script 
In its simplest form, this R script performs a variety of analysis and classification tasks on an automotive accident dataset. It predicts injuries through both exact and naïve Bayesian gets nearer, investigates the predictive power of the naive Bayes classifier, and quantifies the overall error rate in a validation set. The script presents a methodical approach to solving a classification problem in R using Bayesian gets closer.

# Problem Statement

The file accidentsFull.csv contains information on 42,183 actual automobile accidents in 2001 in the United States that involved one of three levels of injury: NO INJURY, INJURY, or FATALITY. For each accident, additional information is recorded, such as day of week, weather conditions, and road type. A firm might be interested in developing a system for quickly classifying the severity of an accident based on initial reports and associated data in the system (some of which rely on GPS-assisted reporting).

Our goal here is to predict whether an accident just reported will involve an injury (MAX_SEV_IR = 1 or 2) or will not (MAX_SEV_IR = 0). For this purpose, create a dummy variable called INJURY that takes the value “yes” if MAX_SEV_IR = 1 or 2, and otherwise “no.”

1. Using the information in this dataset, if an accident has just been reported and no further information is available, what should the prediction be? (INJURY = Yes or No?) Why?
2. Select the first 24 records in the dataset and look only at the response (INJURY) and the two predictors WEATHER_R and TRAF_CON_R. Create a pivot table that examines INJURY as a function of the two predictors for these 12 records. Use all three variables in the pivot table as rows/columns.
  + Compute the exact Bayes conditional probabilities of an injury (INJURY = Yes) given the six possible combinations of the predictors.
  + Classify the 24 accidents using these probabilities and a cutoff of 0.5.
  + Compute manually the naive Bayes conditional probability of an injury given WEATHER_R = 1 and TRAF_CON_R = 1.
  + Run a naive Bayes classifier on the 24 records and two predictors. Check the model output to obtain probabilities and classifications for all 24 records. Compare this to the exact Bayes classification. Are the resulting classifications equivalent? Is the ranking (= ordering) of observations equivalent?
3. Let us now return to the entire dataset. Partition the data into training (60%) and validation (40%). 
  + Run a naive Bayes classifier on the complete training set with the relevant predictors (and INJURY as the response). Note that all predictors are categorical. Show the confusion matrix.
  + What is the overall error of the validation set?

# Summary

## Data Input and Cleaning

Load the required libraries and read the input file
```{r}
library(e1071)
library(caret)
```
```{r}
accidents <- read.csv("C:/Users/gandu/Downloads/accidentsFull.csv")
accidents$INJURY = ifelse(accidents$MAX_SEV_IR>0,"yes","no")


# Convert variables to factor
for (i in c(1:dim(accidents)[2])){
  accidents[,i] <- as.factor(accidents[,i])
}
head(accidents,n=24)
```


## Questions

2. Select the first 24 records in the dataset and look only at the response (INJURY) and the two predictors WEATHER_R and TRAF_CON_R. Create a pivot table that examines INJURY as a function of the two predictors for these 12 records. Use all three variables in the pivot table as rows/columns.
  + Compute the exact Bayes conditional probabilities of an injury (INJURY = Yes) given the six possible combinations of the predictors.
  + Classify the 24 accidents using these probabilities and a cutoff of 0.5.
  + Compute manually the naive Bayes conditional probability of an injury given WEATHER_R = 1 and TRAF_CON_R = 1.
  + Run a naive Bayes classifier on the 24 records and two predictors. Check the model output to obtain probabilities and classifications for all 24 records. Compare this to the exact Bayes classification. Are the resulting classifications equivalent? Is the ranking (= ordering) of observations equivalent?

```{r}
accidents24 <- accidents[1:24,c("INJURY","WEATHER_R","TRAF_CON_R")]
#head(accidents24)
```
```{r}
dt1 <- ftable(accidents24)
dt2 <- ftable(accidents24[,-1]) # print table only for conditions
dt1
dt2
```
2. Select the first 24 records in the dataset and look only at the response (INJURY) and the two predictors WEATHER_R and TRAF_CON_R. Create a pivot table that examines INJURY as a function of the two predictors for these 12 records. Use all three variables in the pivot table as rows/columns.
  + Compute the exact Bayes conditional probabilities of an injury (INJURY = Yes) given the six possible combinations of the predictors.
```{r}
# Injury = yes
p1 = dt1[3,1] / dt2[1,1] # Injury, Weather=1 and Traf=0
p2 = dt1[4,1] / dt2[2,1] # Injury, Weather=2, Traf=0
p3 = dt1[3,2] / dt2[1,2] # Injury, W=1, T=1
p4 = dt1[4,2] / dt2[2,2] # I, W=2,T=1
p5 = dt1[3,3] / dt2[1,3] # I, W=1,T=2
p6 = dt1[4,3]/ dt2[2,3] #I,W=2,T=2

# Injury = no
n1 = dt1[1,1] / dt2[1,1] # Weather=1 and Traf=0
n2 = dt1[2,1] / dt2[2,1] # Weather=2, Traf=0
n3 = dt1[1,2] / dt2[1,2] # W=1, T=1
n4 = dt1[2,2] / dt2[2,2] # W=2,T=1
n5 = dt1[1,3] / dt2[1,3] # W=1,T=2
n6 = dt1[2,3] / dt2[2,3] # W=2,T=2
print(c(p1,p2,p3,p4,p5,p6))
print(c(n1,n2,n3,n4,n5,n6))
```

2. Let us now compute
  + Classify the 24 accidents using these probabilities and a cutoff of 0.5.
```{r}
prob.inj <- rep(0,24)

for (i in 1:24) {
  print(c(accidents24$WEATHER_R[i],accidents24$TRAF_CON_R[i]))
    if (accidents24$WEATHER_R[i] == "1") {
      if (accidents24$TRAF_CON_R[i]=="0"){
        prob.inj[i] = p1
      }
      else if (accidents24$TRAF_CON_R[i]=="1") {
        prob.inj[i] = p3
      }
      else if (accidents24$TRAF_CON_R[i]=="2") {
        prob.inj[i] = p5
      }
    }
    else {
      if (accidents24$TRAF_CON_R[i]=="0"){
        prob.inj[i] = p2
      }
      else if (accidents24$TRAF_CON_R[i]=="1") {
        prob.inj[i] = p4
      }
      else if (accidents24$TRAF_CON_R[i]=="2") {
        prob.inj[i] = p6
      }
    }
  }
  
accidents24$prob.inj <- prob.inj

accidents24$pred.prob <- ifelse(accidents24$prob.inj>0.5, "yes", "no")

```

Compute manually the naive Bayes conditional probability of an injury given WEATHER_R = 1 and TRAF_CON_R = 1.
```{r}

```
 
2. 
  + Run a naive Bayes classifier on the 24 records and two predictors. Check the model output to obtain probabilities and classifications for all 24 records. Compare this to the exact Bayes classification. Are the resulting classifications equivalent? Is the ranking (= ordering) of observations equivalent?
```{r}
nb <- naiveBayes(INJURY ~ TRAF_CON_R + WEATHER_R, 
                 data = accidents24)

nbt <- predict(nb, newdata = accidents24,type = "raw")
accidents24$nbpred.prob <- nbt[,2] # Transfer the "Yes" nb prediction
```
  
  Let us use Caret
```{r}
nb2 <- train(INJURY ~ TRAF_CON_R + WEATHER_R, 
      data = accidents24, method = "nb")

predict(nb2, newdata = accidents24[,c("INJURY", "WEATHER_R", "TRAF_CON_R")])
predict(nb2, newdata = accidents24[,c("INJURY", "WEATHER_R", "TRAF_CON_R")],
                                    type = "raw")
```
  #2(3). Compute manually the naive Bayes conditional probability of an injury given WEATHER_R = 1 and TRAF_CON_R = 1

```{r}

 

# You should load the 'e1071' library to use naiveBayes

library(e1071)

 

# Create a naive Bayes model

nb_model <- naiveBayes(INJURY ~ WEATHER_R + TRAF_CON_R, data = subset_data)

 

# Specify the data for which the probability is to be computed.

new_data <- data.frame(WEATHER_R = "1", TRAF_CON_R = "1")

 

# Predict the probability of "Yes" class

naive_bayes_prob <- predict(nb_model, newdata = new_data, type = "raw")

injury_prob_naive_bayes <- naive_bayes_prob[1, "Yes"]

 

# Print the probability

cat("Naive Bayes Conditional Probability for WEATHER_R = 1 and TRAF_CON_R = 1:\n")

cat(injury_prob_naive_bayes, "\n")

 

```

 

#2(4). Run a naive Bayes classifier on the 24 records and two predictors. Check the model output to obtain probabilities and classifications for all 24 records. Compare this to the exact Bayes classification. Are the resulting classifications equivalent? Is the ranking (= ordering) of observations equivalent?

```{r}

# Load the e1071 library for naiveBayes

library(e1071)

 
#For the 24 records and two predictors, create a naive Bayes model.
nb_model <- train(INJURY ~ TRAF_CON_R + WEATHER_R, data = accidents24, method = "nb")
predictions_caret <- predict(nb_model, newdata = accidents24)

 

# Predict using the naive Bayes model with the same data.

naive_bayes_predictions_24 <- predict(nb_model_24, subset_data)

 

# Extract the probability of "Yes" class for each record

injury_prob_naive_bayes_24 <- attr(naive_bayes_predictions_24, "probabilities")[, "Yes"]

 

# Create a classification vector with a cutoff of 0.5.

classification_results_naive_bayes_24 <- ifelse(injury_prob_naive_bayes_24 > 0.5, "Yes", "No")

 

# Print the classification results

cat("Classification Results based on Naive Bayes for 24 records:\n")

cat(classification_results_naive_bayes_24, sep = " ")

 

# Examine whether the generated classifications are identical to the exact Bayes classification.

equivalent_classifications <- classification_results_naive_bayes_24 == classification_results

 

# Examine whether the ranking (= ordering) of observations is similar.

equivalent_ranking <- all.equal(injury_prob_naive_bayes_24, as.numeric(pivot_table["Yes", , ]))

 

# Print the results of the comparison

cat("\nAre the resulting classifications equivalent? ", all(equivalent_classifications))

cat("\nIs the ranking (= ordering) of observations equivalent? ", equivalent_ranking)

 

 

 

 

```

#3 Let us now return to the entire dataset. Partition the data into training (60%) and validation (40%).

 

#3(1)Run a naive Bayes classifier on the complete training set with the relevant predictors (and INJURY as the response). Note that all predictors are categorical. Show the confusion matrix

 

```{r}

# Load required libraries

library(e1071)

library(caret)

 

# Read the dataset

accidents <- read.csv("C:/Users/gandu/Downloads/accidentsFull.csv")

 

# Make an injury dummy variable.

accidents$INJURY <- ifelse(accidents$MAX_SEV_IR > 0, "Yes", "No")

 

# Change variables to factors

for (i in 1:ncol(accidents)) {

  accidents[[i]] <- as.factor(accidents[[i]])

}

 

# Establish the seed of replicability.
set.seed(123)

 

# Divide the data into training (60%) and validation (40%) sets.

split_index <- createDataPartition(accidents$INJURY, p = 0.6, list = FALSE)

training_data <- accidents[split_index, ]

validation_data <- accidents[-split_index, ]

 

# On the training data, build a naïve Bayes model.

nb_model <- naiveBayes(INJURY ~ ., data = training_data)

 

# Predict on the validation set

nb_predictions <- predict(nb_model, validation_data)

 

# Create a confusion matrix

confusion_matrix <- table(Actual = validation_data$INJURY, Predicted = nb_predictions)

 

# Print the confusion matrix

print(confusion_matrix)

 

```

#3(2)What is the overall error of the validation set?

 

```{r}

# Calculate the overall error

error_rate <- 1 - sum(diag(confusion_matrix)) / sum(confusion_matrix)

cat("Overall error of the validation set:", error_rate, "\n")

 

 

```

 

